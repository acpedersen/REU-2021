{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUPy-2PNar44"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "# from pandas import json_normalize\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from pandas import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16055,
     "status": "ok",
     "timestamp": 1622348881817,
     "user": {
      "displayName": "pooneh mousavi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjfKBf96vNg81URgRBuXe3jg06Kwdmb8gXzWWsc=s64",
      "userId": "07980475777627459411"
     },
     "user_tz": 300
    },
    "id": "xZljJnGCeD8E",
    "outputId": "3ea65892-6eaa-40a9-a92a-d18a4e03518d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# We'll start by creating a directory in which we'll define our new\n",
    "# modules to be imported. Also, we import the data that we need for run the code to \n",
    "# skip the part of generating KNN and Sparse Graph which is the time consuming part.\n",
    "# %mkdir data\n",
    "# %mkdir dataset\n",
    "# %mkdir embeddings\n",
    "%mkdir -p Trec_data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount = True)\n",
    "\n",
    "# %cp -a gdrive/My\\ Drive/VolunteerismTransfer/Code/Labeled.json ./ #May be important\n",
    "%cp -a gdrive/My\\ Drive/VolunteerismTransfer/Code/events_score.csv ./\n",
    "# %cp -a gdrive/My\\ Drive/CorrectAndSmooth/data/. ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xN1oFrq4fGQx"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "import nltk\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.downloader as api\n",
    "import string\n",
    "from sklearn.semi_supervised import LabelSpreading, LabelPropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9jZX9YfUMy-"
   },
   "source": [
    "# **`Clean and Process Data `**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eKHkTWETPan"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhWrJVRHT2oP"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([\"http\", \"https\", \"rt\", \"@\", \":\", \"t.co\", \"co\", \"amp\", \"&amp;\", \"...\", \"\\n\", \"\\r\"])\n",
    "stop_words.extend(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "St4ZCFADpi0k"
   },
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True)\n",
    "        yield(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-g7FfwipkyJ"
   },
   "outputs": [],
   "source": [
    "# Look into what shape this ./abeled.json (misspelled?) is in so it can be replicated\n",
    "df = pd.read_json(\"./abeled.json\", orient='records',lines=True)\n",
    "data = df.text.values.tolist()\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13668,
     "status": "ok",
     "timestamp": 1620330697002,
     "user": {
      "displayName": "Ali Parviz",
      "photoUrl": "",
      "userId": "01225419809509768623"
     },
     "user_tz": 300
    },
    "id": "Dkll3pq2pmRw",
    "outputId": "b225bf6f-ebfd-4138-ecf6-bdedfdc82fc1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYYvQvM4ppYX"
   },
   "outputs": [],
   "source": [
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeOzzAR-pp55"
   },
   "outputs": [],
   "source": [
    "data_ready = process_words(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwLUvM_tpvZa"
   },
   "outputs": [],
   "source": [
    "df['processed_text']=data_ready\n",
    "df.to_json('./PR_all_Labeled.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfcphKyzk8hb"
   },
   "outputs": [],
   "source": [
    "cp -a ./PR_all_Labeled.json  gdrive/My\\ Drive/Code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7s6lda-WFdP"
   },
   "outputs": [],
   "source": [
    "data = pd.read_json(\"./PR_all_Labeled.json\", orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WizZdqM-VD0q"
   },
   "source": [
    "# Generating Similarity Scores and Matrix\n",
    "### **Mean and Cosine Similarity(each event with all other event-types)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6XJslCRWeE-"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import logging\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import os.path\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7ovyMf7WkJN"
   },
   "outputs": [],
   "source": [
    "# remove irrelevant tweets\n",
    "data = pd.read_json(\"./PR_all_Labeled.json\", orient='records',lines=True)\n",
    "data.loc[(data.eventid == 'parisAttacks2015'),'event_type']='shooting'\n",
    "def label_ir_tweets(postCategories):\n",
    "\n",
    "    if 'Irrelevant' in postCategories:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "data['ir'] = [label_ir_tweets(x) for x in data['categories']]\n",
    "data=data.query(\"ir == 0\")\n",
    "data=data.query(\"label ==1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qcaVhhLWlZe"
   },
   "outputs": [],
   "source": [
    "def label_observation_tweets(postCategories):\n",
    "    if 'FirstPartyObservation' in postCategories or 'ThirdPartyObservation' in postCategories :\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "data['obs'] = [label_observation_tweets(x) for x in data['categories']]\n",
    "data=data.query(\"obs == 1\")\n",
    "# data=data.query(\"label ==1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVUJ_QksWqn6"
   },
   "outputs": [],
   "source": [
    "data['l'] = data.apply(lambda row: len(row['processed_text']), axis=1)\n",
    "data= data.query(\"l >1\")\n",
    "data.drop(columns=['l'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R70lzr6FWwMk"
   },
   "outputs": [],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "882RiHajWysw"
   },
   "outputs": [],
   "source": [
    "# generate sentnece embedding\n",
    "class SBERT:\n",
    "\n",
    "    def __init__(self, lang=\"en\"):\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        self.name = \"SBERT\"\n",
    "        if lang == \"fr\":\n",
    "            self.model = SentenceTransformer(\n",
    "                \"/home/bmazoyer/Dev/pytorch_bert/output/sts_fr_long_multilingual_bert-2019-10-01_15-07-03\")\n",
    "        elif lang == \"en\":\n",
    "            self.model = SentenceTransformer(\n",
    "                # \"bert-large-nli-stsb-mean-tokens\"\n",
    "                \"roberta-large-nli-stsb-mean-tokens\"\n",
    "            )\n",
    "# roberta-large-nli-stsb-mean-tokens\n",
    "    def compute_vectors(self, data):\n",
    "        data[\"text\"] = data.text.str.slice(0, 500)\n",
    "        vectors = np.array(self.model.encode(data.text.tolist()))\n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nv3JJ_CJW1JX"
   },
   "outputs": [],
   "source": [
    "sbert=SBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IDPnWRcW5PK"
   },
   "outputs": [],
   "source": [
    "v=sbert.compute_vectors(data)\n",
    "data['sbert_emb']=[item for item in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2x3adgMW-y_"
   },
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QT05N6hhXAac"
   },
   "outputs": [],
   "source": [
    "def generate_similarity_matrix (frame, grouping, group_types):\n",
    "    #generate similarity scores dataframe\n",
    "    group_ranks=pd.DataFrame()\n",
    "    groupby_col=grouping\n",
    "    group_ranks=frame.query(\"label==1\").groupby(grouping) #Does the label==1 need to change?\n",
    "    for heldout_event,g in group_types:\n",
    "\n",
    "        training = frame[frame[groupby_col] != heldout_event]\n",
    "        test = frame[frame[groupby_col] == heldout_event]\n",
    "\n",
    "        ref=np.mean(test[\"sbert_emb\"], axis=0)\n",
    "\n",
    "        grpups=training.query(\"label==1\").groupby(grouping)\n",
    "        ranks={}\n",
    "        ranks[\"reference-group\"]=heldout_event\n",
    "        for name, group in grpups:\n",
    "           val=np.mean(group[\"sbert_emb\"], axis=0)\n",
    "           cos_sim = dot(ref, val)/(norm(ref)*norm(val))\n",
    "           ranks[name]=cos_sim\n",
    "\n",
    "        # event_ranks[heldout_event]=ranks\n",
    "        group_ranks = group_ranks.append(ranks, ignore_index=True)\n",
    "    group_ranks.set_index(\"reference-group\",inplace=True)\n",
    "    return group_ranks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5k3ms2FW9My"
   },
   "outputs": [],
   "source": [
    "#is this even required?\n",
    "events=[ \n",
    "'2014_Philippines_Typhoon_Hagupi',\n",
    " '2015_Cyclone_Pam',\n",
    " 'albertaFloods2013',\n",
    " 'albertaWildfires2019',\n",
    " 'australiaBushfire2013',\n",
    " 'cycloneKenneth2019',\n",
    " 'fireYMM2016',\n",
    " 'hurricaneFlorence2018',\n",
    " 'keralaFloods2019',\n",
    " 'manilaFloods2013',\n",
    " 'philipinnesFloods2012',\n",
    " 'queenslandFloods2013',\n",
    " 'southAfricaFloods2019',\n",
    " 'typhoonHagupit2014',\n",
    " 'typhoonYolanda2013'\n",
    "]\n",
    "event_types=[ 'hurricane/typhoon/cyclone/tornado','flood','wildfire/bushfire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyO1rHEeXC7D"
   },
   "outputs": [],
   "source": [
    "event_ranks = generate_similarity_matrix(data, 'event_type', event_types)\n",
    "event_ranks.to_csv(\"./event_ranks.csv\")\n",
    "#cp -a ./event_ranks_roberta.csv gdrive/My\\ Drive/Code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZCJ-jZfXGfk"
   },
   "outputs": [],
   "source": [
    "# Visualize similarity matrix for event type using heatmap\n",
    "event_ranks.set_index(\"reference-group\",inplace=True)\n",
    "p1 = sns.heatmap(event_ranks,cmap=\"YlGnBu\", annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_types=[''] #double check that this is string based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyO1rHEeXC7D"
   },
   "outputs": [],
   "source": [
    "info_ranks = generate_similarity_matrix(data, 'info_type', info_types) #check what this variable is called in the dataframe\n",
    "info_ranks.to_csv(\"./info_ranks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZCJ-jZfXGfk"
   },
   "outputs": [],
   "source": [
    "# Visualize similarity matrix for info type using heatmap\n",
    "info_ranks.set_index(\"reference-group\",inplace=True)\n",
    "p2 = sns.heatmap(info_ranks,cmap=\"YlGnBu\", annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critical_types=[0, 1, 2, 3] #double check this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyO1rHEeXC7D"
   },
   "outputs": [],
   "source": [
    "critical_ranks = generate_similarity_matrix(data, 'criticality_type', critical_types) #check what this variable is called in the dataframe\n",
    "critical_ranks.to_csv(\"./critical_ranks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZCJ-jZfXGfk"
   },
   "outputs": [],
   "source": [
    "# Visualize similarity matrix for info type using heatmap\n",
    "critical_ranks.set_index(\"reference-group\",inplace=True)\n",
    "p3 = sns.heatmap(critical_ranks,cmap=\"YlGnBu\", annot=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "o9jZX9YfUMy-",
    "ZCsv8uqlUZjT"
   ],
   "machine_shape": "hm",
   "name": "analyze-data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
