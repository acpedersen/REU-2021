{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RUPy-2PNar44"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "#from pandas import json_normalize #Deprecated? Also not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xN1oFrq4fGQx"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess#, lemmatize deprecetated\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.downloader as api\n",
    "import string\n",
    "from sklearn.semi_supervised import LabelSpreading, LabelPropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9jZX9YfUMy-"
   },
   "source": [
    "# **`Clean and Process Data `**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3eKHkTWETPan"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> stopword\n",
      "Command 'stopword' unrecognized\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> stopwords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Downloading package stopwords to /home/g/g01107/nltk_data...\n",
      "      Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download() #Dowload stopwords, only needed to update after first download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventID</th>\n",
       "      <th>eventType</th>\n",
       "      <th>postID</th>\n",
       "      <th>postCategories</th>\n",
       "      <th>postPriority</th>\n",
       "      <th>postText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stormJorge2020</td>\n",
       "      <td>typhoon</td>\n",
       "      <td>1231307896362807298</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>Low</td>\n",
       "      <td>Flood Warning: River Severn at Hanley Castle a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stormJorge2020</td>\n",
       "      <td>typhoon</td>\n",
       "      <td>1231569665043976192</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>Low</td>\n",
       "      <td>Flood Warning: River Ouse at Naburn Lock 12:46...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stormJorge2020</td>\n",
       "      <td>typhoon</td>\n",
       "      <td>1232264304067477504</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>Low</td>\n",
       "      <td>Our Assistant Director of Care and Support kin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stormJorge2020</td>\n",
       "      <td>typhoon</td>\n",
       "      <td>1232070602778959872</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>Low</td>\n",
       "      <td>@hollywills please can you help support @HopeR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stormJorge2020</td>\n",
       "      <td>typhoon</td>\n",
       "      <td>1232648900105965568</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>Low</td>\n",
       "      <td>Police order 'immediate evacuation' in Shropsh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91510</th>\n",
       "      <td>whaleyBridgeCollapse2020</td>\n",
       "      <td>flood</td>\n",
       "      <td>1155430270457323520</td>\n",
       "      <td>[Irrelevant]</td>\n",
       "      <td>Low</td>\n",
       "      <td>Flood Alert: River Ecclesbourne in Derbyshire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91511</th>\n",
       "      <td>whaleyBridgeCollapse2020</td>\n",
       "      <td>flood</td>\n",
       "      <td>1156993824591417346</td>\n",
       "      <td>[Location, EmergingThreats, MultimediaShare, N...</td>\n",
       "      <td>High</td>\n",
       "      <td>Dam at Whaley Bridge in Peak District threaten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91512</th>\n",
       "      <td>whaleyBridgeCollapse2020</td>\n",
       "      <td>flood</td>\n",
       "      <td>1157020257388769280</td>\n",
       "      <td>[ThirdPartyObservation, Location, MultimediaSh...</td>\n",
       "      <td>Low</td>\n",
       "      <td>Floods in Whaley Bridge today.\\nhttps://t.co/7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91513</th>\n",
       "      <td>whaleyBridgeCollapse2020</td>\n",
       "      <td>flood</td>\n",
       "      <td>1156926115069485056</td>\n",
       "      <td>[MovePeople, ThirdPartyObservation, Location, ...</td>\n",
       "      <td>Critical</td>\n",
       "      <td>Evacuation of Whaley Bridge | Derbyshire Const...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91514</th>\n",
       "      <td>whaleyBridgeCollapse2020</td>\n",
       "      <td>flood</td>\n",
       "      <td>1157001345649532935</td>\n",
       "      <td>[Hashtags, Sentiment]</td>\n",
       "      <td>Low</td>\n",
       "      <td>@itvnews Praying all are safe #WhaleyBridge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91515 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        eventID eventType               postID  \\\n",
       "0                stormJorge2020   typhoon  1231307896362807298   \n",
       "1                stormJorge2020   typhoon  1231569665043976192   \n",
       "2                stormJorge2020   typhoon  1232264304067477504   \n",
       "3                stormJorge2020   typhoon  1232070602778959872   \n",
       "4                stormJorge2020   typhoon  1232648900105965568   \n",
       "...                         ...       ...                  ...   \n",
       "91510  whaleyBridgeCollapse2020     flood  1155430270457323520   \n",
       "91511  whaleyBridgeCollapse2020     flood  1156993824591417346   \n",
       "91512  whaleyBridgeCollapse2020     flood  1157020257388769280   \n",
       "91513  whaleyBridgeCollapse2020     flood  1156926115069485056   \n",
       "91514  whaleyBridgeCollapse2020     flood  1157001345649532935   \n",
       "\n",
       "                                          postCategories postPriority  \\\n",
       "0                                           [Irrelevant]          Low   \n",
       "1                                           [Irrelevant]          Low   \n",
       "2                                           [Irrelevant]          Low   \n",
       "3                                           [Irrelevant]          Low   \n",
       "4                                           [Irrelevant]          Low   \n",
       "...                                                  ...          ...   \n",
       "91510                                       [Irrelevant]          Low   \n",
       "91511  [Location, EmergingThreats, MultimediaShare, N...         High   \n",
       "91512  [ThirdPartyObservation, Location, MultimediaSh...          Low   \n",
       "91513  [MovePeople, ThirdPartyObservation, Location, ...     Critical   \n",
       "91514                              [Hashtags, Sentiment]          Low   \n",
       "\n",
       "                                                postText  \n",
       "0      Flood Warning: River Severn at Hanley Castle a...  \n",
       "1      Flood Warning: River Ouse at Naburn Lock 12:46...  \n",
       "2      Our Assistant Director of Care and Support kin...  \n",
       "3      @hollywills please can you help support @HopeR...  \n",
       "4      Police order 'immediate evacuation' in Shropsh...  \n",
       "...                                                  ...  \n",
       "91510  Flood Alert: River Ecclesbourne in Derbyshire ...  \n",
       "91511  Dam at Whaley Bridge in Peak District threaten...  \n",
       "91512  Floods in Whaley Bridge today.\\nhttps://t.co/7...  \n",
       "91513  Evacuation of Whaley Bridge | Derbyshire Const...  \n",
       "91514        @itvnews Praying all are safe #WhaleyBridge  \n",
       "\n",
       "[91515 rows x 6 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Opens the data and reorganizes it by tweets instead of by events\n",
    "with open('./Trec_data/labeled_by_event.json') as f:\n",
    "    js = json.load(f)\n",
    "df = json_normalize(data=js['events'], record_path='tweets')\n",
    "df.to_json('./Trec_data/labeled.json', orient='records', lines=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "GhWrJVRHT2oP"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([\"http\", \"https\", \"rt\", \"@\", \":\", \"t.co\", \"co\", \"amp\", \"&amp;\", \"...\", \"\\n\", \"\\r\"])\n",
    "stop_words.extend(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "St4ZCFADpi0k"
   },
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        if sent: #Prevent blank text from appearing\n",
    "            sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "            sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "            sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "            sent = gensim.utils.simple_preprocess(str(sent), deacc=True)\n",
    "            yield(sent)\n",
    "        else:\n",
    "            #Do nothing I guess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "k-g7FfwipkyJ"
   },
   "outputs": [],
   "source": [
    "# Look into what shape this ./labeled.json\n",
    "df = pd.read_json(\"./Trec_data/labeled.json\", orient='records', lines=True)\n",
    "data = df.postText.values.tolist()\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13668,
     "status": "ok",
     "timestamp": 1620330697002,
     "user": {
      "displayName": "Ali Parviz",
      "photoUrl": "",
      "userId": "01225419809509768623"
     },
     "user_tz": 300
    },
    "id": "Dkll3pq2pmRw",
    "outputId": "b225bf6f-ebfd-4138-ecf6-bdedfdc82fc1"
   },
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "FYYvQvM4ppYX"
   },
   "outputs": [],
   "source": [
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner']) #Change en to be full name\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "XeOzzAR-pp55"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"en\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-6a967a6d9ec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-87-2d98d0bbc26c>\u001b[0m in \u001b[0;36mprocess_words\u001b[0;34m(texts, stop_words, allowed_postags)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrigram_mod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbigram_mod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtexts_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parser'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ner'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \"\"\"\n\u001b[0;32m---> 50\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"en\")"
     ]
    }
   ],
   "source": [
    "data_ready = process_words(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwLUvM_tpvZa"
   },
   "outputs": [],
   "source": [
    "df['processed_text']=data_ready\n",
    "df.to_json('./Trec_data/PR_all_Labeled.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfcphKyzk8hb"
   },
   "outputs": [],
   "source": [
    "#What does this do?\n",
    "#cp -a ./PR_all_Labeled.json  gdrive/My\\ Drive/Code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7s6lda-WFdP"
   },
   "outputs": [],
   "source": [
    "data = pd.read_json(\"./Trec_data/PR_all_Labeled.json\", orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WizZdqM-VD0q"
   },
   "source": [
    "# Generating Similarity Scores and Matrix\n",
    "### **Mean and Cosine Similarity(each event with all other event-types)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6XJslCRWeE-"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#import logging\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import os.path\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7ovyMf7WkJN"
   },
   "outputs": [],
   "source": [
    "# remove irrelevant tweets\n",
    "data = pd.read_json(\"./Trec_data/PR_all_Labeled.json\", orient='records',lines=True)\n",
    "data.loc[(data.eventid == 'parisAttacks2015'),'event_type']='shooting'\n",
    "def label_ir_tweets(postCategories):\n",
    "\n",
    "    if 'Irrelevant' in postCategories:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "data['ir'] = [label_ir_tweets(x) for x in data['categories']]\n",
    "data=data.query(\"ir == 0\")\n",
    "data=data.query(\"label ==1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qcaVhhLWlZe"
   },
   "outputs": [],
   "source": [
    "#This only removes all tweets except ovbservations\n",
    "#Do not run but keep for reference\n",
    "def label_observation_tweets(postCategories):\n",
    "    if 'FirstPartyObservation' in postCategories or 'ThirdPartyObservation' in postCategories :\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "#data['obs'] = [label_observation_tweets(x) for x in data['categories']]\n",
    "#data=data.query(\"obs == 1\")\n",
    "# data=data.query(\"label ==1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVUJ_QksWqn6"
   },
   "outputs": [],
   "source": [
    "data['l'] = data.apply(lambda row: len(row['processed_text']), axis=1)\n",
    "data= data.query(\"l >1\")\n",
    "data.drop(columns=['l'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R70lzr6FWwMk"
   },
   "outputs": [],
   "source": [
    "#This would run through the ssh oopsy, do not run\n",
    "#pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "882RiHajWysw"
   },
   "outputs": [],
   "source": [
    "# generate sentnece embedding\n",
    "class SBERT:\n",
    "\n",
    "    def __init__(self, lang=\"en\"):\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        self.name = \"SBERT\"\n",
    "        if lang == \"fr\":\n",
    "            self.model = SentenceTransformer(\n",
    "                \"/home/bmazoyer/Dev/pytorch_bert/output/sts_fr_long_multilingual_bert-2019-10-01_15-07-03\")\n",
    "        elif lang == \"en\":\n",
    "            self.model = SentenceTransformer(\n",
    "                # \"bert-large-nli-stsb-mean-tokens\"\n",
    "                \"roberta-large-nli-stsb-mean-tokens\"\n",
    "            )\n",
    "# roberta-large-nli-stsb-mean-tokens\n",
    "    def compute_vectors(self, data):\n",
    "        data[\"text\"] = data.text.str.slice(0, 500)\n",
    "        vectors = np.array(self.model.encode(data.text.tolist()))\n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nv3JJ_CJW1JX"
   },
   "outputs": [],
   "source": [
    "sbert=SBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IDPnWRcW5PK"
   },
   "outputs": [],
   "source": [
    "v=sbert.compute_vectors(data)\n",
    "data['sbert_emb']=[item for item in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2x3adgMW-y_"
   },
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QT05N6hhXAac"
   },
   "outputs": [],
   "source": [
    "def generate_similarity_matrix (frame, grouping, group_types):\n",
    "    #generate similarity scores dataframe\n",
    "    group_ranks=pd.DataFrame()\n",
    "    groupby_col=grouping\n",
    "    group_ranks=frame.query(\"label==1\").groupby(grouping) #Does the label==1 need to change?\n",
    "    for heldout_event,g in group_types:\n",
    "\n",
    "        training = frame[frame[groupby_col] != heldout_event]\n",
    "        test = frame[frame[groupby_col] == heldout_event]\n",
    "\n",
    "        ref=np.mean(test[\"sbert_emb\"], axis=0)\n",
    "\n",
    "        grpups=training.query(\"label==1\").groupby(grouping)\n",
    "        ranks={}\n",
    "        ranks[\"reference-group\"]=heldout_event\n",
    "        for name, group in grpups:\n",
    "           val=np.mean(group[\"sbert_emb\"], axis=0)\n",
    "           cos_sim = dot(ref, val)/(norm(ref)*norm(val))\n",
    "           ranks[name]=cos_sim\n",
    "\n",
    "        # event_ranks[heldout_event]=ranks\n",
    "        group_ranks = group_ranks.append(ranks, ignore_index=True)\n",
    "    group_ranks.set_index(\"reference-group\",inplace=True)\n",
    "    return group_ranks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5k3ms2FW9My"
   },
   "outputs": [],
   "source": [
    "#is this even required? and is this based off the 4 different sources?\n",
    "events=[ \n",
    "'2014_Philippines_Typhoon_Hagupi',\n",
    " '2015_Cyclone_Pam',\n",
    " 'albertaFloods2013',\n",
    " 'albertaWildfires2019',\n",
    " 'australiaBushfire2013',\n",
    " 'cycloneKenneth2019',\n",
    " 'fireYMM2016',\n",
    " 'hurricaneFlorence2018',\n",
    " 'keralaFloods2019',\n",
    " 'manilaFloods2013',\n",
    " 'philipinnesFloods2012',\n",
    " 'queenslandFloods2013',\n",
    " 'southAfricaFloods2019',\n",
    " 'typhoonHagupit2014',\n",
    " 'typhoonYolanda2013'\n",
    "]\n",
    "event_types=[ 'hurricane/typhoon/cyclone/tornado','flood','wildfire/bushfire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyO1rHEeXC7D"
   },
   "outputs": [],
   "source": [
    "event_ranks = generate_similarity_matrix(data, 'event_type', event_types)\n",
    "event_ranks.to_csv(\"./Trec_data/event_ranks.csv\")\n",
    "#cp -a ./event_ranks_roberta.csv gdrive/My\\ Drive/Code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZCJ-jZfXGfk"
   },
   "outputs": [],
   "source": [
    "# Visualize similarity matrix for event type using heatmap\n",
    "event_ranks.set_index(\"reference-group\",inplace=True)\n",
    "p1 = sns.heatmap(event_ranks,cmap=\"YlGnBu\", annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_types=[''] #double check that this is string based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyO1rHEeXC7D"
   },
   "outputs": [],
   "source": [
    "info_ranks = generate_similarity_matrix(data, 'info_type', info_types) #check what this variable is called in the dataframe\n",
    "info_ranks.to_csv(\"./Trec_data/info_ranks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZCJ-jZfXGfk"
   },
   "outputs": [],
   "source": [
    "# Visualize similarity matrix for info type using heatmap\n",
    "info_ranks.set_index(\"reference-group\",inplace=True)\n",
    "p2 = sns.heatmap(info_ranks,cmap=\"YlGnBu\", annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critical_types=['Low', 'Medium', 'High', 'Critical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyO1rHEeXC7D"
   },
   "outputs": [],
   "source": [
    "critical_ranks = generate_similarity_matrix(data, 'criticality_type', critical_types) #check what this variable is called in the dataframe\n",
    "critical_ranks.to_csv(\"./Trec_data/critical_ranks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZCJ-jZfXGfk"
   },
   "outputs": [],
   "source": [
    "# Visualize similarity matrix for info type using heatmap\n",
    "critical_ranks.set_index(\"reference-group\",inplace=True)\n",
    "p3 = sns.heatmap(critical_ranks,cmap=\"YlGnBu\", annot=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "o9jZX9YfUMy-",
    "ZCsv8uqlUZjT"
   ],
   "machine_shape": "hm",
   "name": "analyze-data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
